import dataclasses
import getpass
import grp
import logging
import os
import pathlib
import queue
import re
import shutil
import tempfile
import threading
import time
from datetime import datetime
from typing import List
import json

from vegvisir.configuration import Configuration
from vegvisir.data import VegvisirArguments
from vegvisir.environments.base_environment import BaseEnvironment
from vegvisir.exceptions import VegvisirException, VegvisirRunFailedException
from vegvisir.hostinterface import HostInterface

from .implementation import Endpoint, Parameters


class LogFileFormatter(logging.Formatter):
	def format(self, record):
		msg = super(LogFileFormatter, self).format(record)
		# remove color control characters
		return re.compile(r"\x1B[@-_][0-?]*[ -/]*[@-~]").sub("", msg)

class Experiment:
	def __init__(self, sudo_password: str, configuration_object: Configuration):
		self.configuration = configuration_object

		self.post_hook_processors: List[threading.Thread] = []
		self.post_hook_processor_request_stop: bool = False
		self.post_hook_processor_queue: queue.Queue = queue.Queue()  # contains tuples (method pointer, path dataclass)

		self.host_interface = HostInterface(sudo_password)

		self.logger = logging.getLogger("root.Experiment")

		# Explicit check so we don't keep trigger an auth lock
		if not self.host_interface._is_sudo_password_valid():
			raise VegvisirException("Authentication with sudo failed. Provided password is wrong?")

	def _post_hook_processor(self):
		while not self.post_hook_processor_request_stop:
			try:
				task, experiment_paths = self.post_hook_processor_queue.get(timeout=5)
				try:
					task(experiment_paths)
				except Exception as e:
					self.logger.error(f"Post-hook encountered an exception | {e}")
			except queue.Empty:
				pass  # We can ignore this one


	def _enable_ipv6(self):
		"""
		sudo modprobe ip6table_filter
		"""
		_, out, err = self.host_interface.spawn_blocking_subprocess("modprobe ip6table_filter", True, False)
		if out != "" or err != "":
			self.logger.debug(f"Enabling ipv6 resulted in non empty output | STDOUT [{out}] | STDERR [{err}]")

	def print_debug_information(self, command: str) -> None:
		_, out, err = self.host_interface.spawn_blocking_subprocess(command, True, False)
		self.logger.debug(f"Command [{command}]:\n{out}")
		if err is not None and len(err) > 0:
			self.logger.warning(f"Command [{command}] returned stderr output:\n{err}")

	def _write_multiflow_compose_override(self, flow_labels: List[str]):
		"""
		Write docker-compose.override.yml that defines one client_<label> service per flow.
		Each service extends 'client' from the base compose file, uses its own env file,
		and gets a static IP on 'leftnet'. IPv4/IPv6 are quoted for YAML safety.
		"""
		base_compose = "./docker-compose.yml"  # adjust if your base file lives elsewhere

		lines = []
		# Optional but harmless with Compose v2: being explicit keeps parsers happy
		# lines.append('version: "3.9"')
		lines.append("services:")

		for idx, label in enumerate(flow_labels, start=1):
			ipv4 = f"193.167.0.{2 + idx}"
			ipv6 = f"fd00:cafe:cafe:0::{2 + idx}"

			lines += [
				f"  client_{label}:",
				"    extends:",
				f"      file: {base_compose}",
				"      service: client",
				f"    container_name: client_{label}",
				"    env_file:",
				f"      - client_{label}.env",
				"    networks:",
				"      leftnet:",
				f'        ipv4_address: "{ipv4}"',
				f'        ipv6_address: "{ipv6}"',
			]

		override_path = "docker-compose.override.yml"
		with open(override_path, "w") as fp:
			fp.write("\n".join(lines) + "\n")
		return override_path

	def run(self):
		vegvisir_start_time = datetime.now()

		# Root path for logs needs to be known and exist for metadata copies
		self.configuration.path_collection.log_path_date = os.path.join(self.configuration.path_collection.log_path_root, "{:%Y-%m-%dT_%H-%M-%S}".format(vegvisir_start_time))
		pathlib.Path(self.configuration.path_collection.log_path_date).mkdir(parents=True, exist_ok=True)
		
		# Copy the implementations and experiment configurations for reproducibility purposes
		# For now, assume json files
		implementations_destination = os.path.join(self.configuration.path_collection.log_path_date, "implementations.json")
		experiment_destination = os.path.join(self.configuration.path_collection.log_path_date, "experiment.json")
		try:
			shutil.copy2(self.configuration.path_collection.implementations_configuration_file_path, implementations_destination) 
		except IOError as e:
			self.logger.warning(f"Could not copy over implementations configuration to root of experiment logs: {implementations_destination} | {e}")
		try:
			shutil.copy2(self.configuration.path_collection.experiment_configuration_file_path, experiment_destination) 
		except IOError as e:
			self.logger.warning(f"Could not copy over experiment configuration to root of experiment logs: {experiment_destination} | {e}")

		for _ in range(max(1, self.configuration.hook_processor_count)):
			processor = threading.Thread(target=self._post_hook_processor)
			processor.start()
			self.post_hook_processors.append(processor)

		self._enable_ipv6()

		experiment_permutation_total = len(self.configuration.shaper_configurations) * len(self.configuration.server_configurations) * len(self.configuration.client_configurations) * self.configuration.iterations
		experiment_permutation_counter = 0
		for shaper_config in self.configuration.shaper_configurations:
			for server_config in self.configuration.server_configurations:
				for client_config in self.configuration.client_configurations:
					yield client_config["name"], shaper_config["name"], server_config["name"], experiment_permutation_counter, experiment_permutation_total
					self.logger.info(f'Running {client_config["name"]} over {shaper_config["name"]} against {server_config["name"]}')
					shaper = self.configuration.shapers[shaper_config["name"]]
					server = self.configuration.server_endpoints[server_config["name"]]
					client = self.configuration.client_endpoints[client_config["name"]]

					# SETUP
					if client.type == Endpoint.Type.HOST:
						_, out, err = self.host_interface.spawn_blocking_subprocess("hostman add 193.167.100.100 server4", True, False)
						self.logger.debug("Vegvisir: append entry to hosts: %s", out.strip())
						if err is not None and len(err) > 0:
							self.logger.debug("Vegvisir: appending entry to hosts file resulted in error: %s", err)

					for run_number in range(0,self.configuration.iterations):
						iteration_start_time = datetime.now()
						
						# Paths, we create the folders so we can later bind them as docker volumes for direct logging output
						# Avoids docker "no space left on device" errors
						self.configuration.path_collection.log_path_iteration = os.path.join(self.configuration.path_collection.log_path_date, f"run_{run_number}/") if self.configuration.iterations > 1 else self.configuration.path_collection.log_path_date
						self.configuration.path_collection.log_path_permutation = os.path.join(self.configuration.path_collection.log_path_iteration, f"{client_config.get('log_name', client_config['name'])}__{shaper_config.get('log_name', shaper_config['name'])}__{server_config.get('log_name', server_config['name'])}")
						self.configuration.path_collection.log_path_client = os.path.join(self.configuration.path_collection.log_path_permutation, 'client')
						self.configuration.path_collection.log_path_server = os.path.join(self.configuration.path_collection.log_path_permutation, 'server')
						self.configuration.path_collection.log_path_shaper = os.path.join(self.configuration.path_collection.log_path_permutation, 'shaper')
						self.configuration.path_collection.download_path_client = os.path.join(self.configuration.path_collection.log_path_permutation, 'downloads')
						for log_dir in [self.configuration.path_collection.log_path_client, self.configuration.path_collection.log_path_server, self.configuration.path_collection.log_path_shaper, self.configuration.path_collection.download_path_client]:
							pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)
						pathlib.Path(os.path.join(self.configuration.path_collection.log_path_iteration, "client__shaper__server")).touch()						

						# We want all output to be saved to file for later evaluation/debugging
						log_file = os.path.join(self.configuration.path_collection.log_path_permutation, "output.txt")
						log_handler = logging.FileHandler(log_file)
						log_handler.setLevel(logging.DEBUG)
						self.logger.addHandler(log_handler)

						path_collection_copy = dataclasses.replace(self.configuration.path_collection)

						self.logger.debug("Calling environment pre_hook")
						pre_hook_start = datetime.now()
						try:
							self.configuration.environment.pre_run_hook(path_collection_copy)
							pre_hook_total = datetime.now() - pre_hook_start
							if pre_hook_total.total_seconds() > 5:
								self.logger.debug(f"Pre-hook took {datetime.now() - pre_hook_start} to complete.")
						except Exception as e:
							self.logger.error(f"Pre-hook encountered an exception | {e}")

						vegvisirBaseArguments = VegvisirArguments()
						vegvisirBaseArguments.LOG_PATH_CLIENT = self.configuration.path_collection.log_path_client
						vegvisirBaseArguments.LOG_PATH_SERVER = self.configuration.path_collection.log_path_server
						vegvisirBaseArguments.LOG_PATH_SHAPER = self.configuration.path_collection.log_path_shaper
						vegvisirBaseArguments.DOWNLOAD_PATH_CLIENT = self.configuration.path_collection.download_path_client

						client_image = client.image.full if client.type == Endpoint.Type.DOCKER else "none"  # Docker compose v2 requires an image name, can't default to blank string

						cert_path = tempfile.TemporaryDirectory(dir="/tmp", prefix="vegvisir_certs_")
						vegvisirBaseArguments.CERT_FINGERPRINT = self.configuration.environment.generate_cert_chain(cert_path.name)

						# TODO pick a better/cleaner spot to do this
						vegvisirBaseArguments.ORIGIN = "server4"
						vegvisirBaseArguments.ORIGIN_IPV4 = "server4"
						vegvisirBaseArguments.ORIGIN_IPV6 = "server6" # TODO hostman this
						vegvisirBaseArguments.ORIGIN_PORT = "443"
						vegvisirBaseArguments.WAITFORSERVER = "server4:443"
						vegvisirBaseArguments.SSLKEYLOGFILE = "/logs/keys.log"
						vegvisirBaseArguments.QLOGDIR = "/logs/qlog/"
						vegvisirBaseArguments.ENVIRONMENT = self.configuration.environment.environment_name if self.configuration.environment.environment_name != "" else None

						vegvisirServerArguments = dataclasses.replace(vegvisirBaseArguments, ROLE="server", TESTCASE=self.configuration.environment.get_QIR_compatibility_testcase(BaseEnvironment.Perspective.SERVER))
						vegvisirShaperArguments = dataclasses.replace(vegvisirBaseArguments, ROLE="shaper", SCENARIO = shaper.scenarios[shaper_config["scenario"]].command, WAITFORSERVER="server:443")  # Important edgecase! Shaper uses server instead of server4

						docker_compose_vars = (
							"CLIENT=" + client_image + " "
							"SERVER=" + server.image.full + " "
							"SHAPER=" + shaper.image.full + " "

							"CERTS=" + cert_path.name + " "
							"WWW=" + self.configuration.www_path + " "
							"DOWNLOAD_PATH_CLIENT=\"" + self.configuration.path_collection.download_path_client + "\" "

							"LOG_PATH_CLIENT=\"" + self.configuration.path_collection.log_path_client + "\" "
							"LOG_PATH_SERVER=\"" + self.configuration.path_collection.log_path_server + "\" "
							"LOG_PATH_SHAPER=\"" + self.configuration.path_collection.log_path_shaper + "\" "
						)

						
						# server_params = server.parameters.hydrate_with_arguments(server_config.get("arguments", {}), {"ROLE": "server", "SSLKEYLOGFILE": "/logs/keys.log", "QLOGDIR": "/logs/qlog/", "TESTCASE": self.configuration.environment.get_QIR_compatibility_testcase(BaseEnvironment.Perspective.SERVER)})
						server_params = server.parameters.hydrate_with_arguments(server_config.get("arguments", {}), vegvisirServerArguments.dict())
						# shaper_params = shaper.scenarios[shaper_config["scenario"]].parameters.hydrate_with_arguments(shaper_config.get("arguments", {}), {"WAITFORSERVER": "server:443", "SCENARIO": shaper.scenarios[shaper_config["scenario"]].command})
						shaper_params = shaper.scenarios[shaper_config["scenario"]].parameters.hydrate_with_arguments(shaper_config.get("arguments", {}), vegvisirShaperArguments.dict())
						
						
						# BEGIN multi-flow manifest emission (flows.json) and FLOWS_PATH injection
						try:
							is_multi = isinstance(client_config.get("multi"), dict) and isinstance(client_config["multi"].get("flows"), list) and len(client_config["multi"]["flows"]) > 0
							is_multi_flows_scenario = (shaper_config.get("scenario") == "multi_flows")
							if is_multi and is_multi_flows_scenario:
								flows = []
								for flow in client_config["multi"]["flows"]:
									label = flow["label"]
									netem = (flow.get("netem") or {})
									# accept both extra_delay_ms and extra_latency_ms
									extra_delay = netem.get("extra_delay_ms", netem.get("extra_latency_ms", 0))
									loss_pct = netem.get("loss_pct", 0)
									flows.append({
										"label": label,
										"container": f"client_{label}",
										"extra_delay_ms": int(extra_delay),
										"loss_pct": float(loss_pct)
									})

								# Base properties from shaper arguments
								base_latency = int(shaper_config.get("arguments", {}).get("LATENCY", 0))
								base_rate    = int(shaper_config.get("arguments", {}).get("THROUGHPUT", 0))

								manifest = {
									"base": {
										"latency_ms": base_latency,
										"throughput_mbps": base_rate
									},
									"server_container": "server",
									"flows": flows
								}

								# Write to the shaper log dir (mounted as /logs inside the shaper)
								flows_host_path = os.path.join(self.configuration.path_collection.log_path_shaper, "flows.json")
								pathlib.Path(self.configuration.path_collection.log_path_shaper).mkdir(parents=True, exist_ok=True)
								with open(flows_host_path, "w") as _fp:
									json.dump(manifest, _fp, indent=2)

								# Ensure FLOWS_PATH is provided to the shaper
								try:
									# shaper_params may be a dict-like mapping
									if isinstance(shaper_params, dict) and "FLOWS_PATH" not in shaper_params:
										shaper_params["FLOWS_PATH"] = "/logs/flows.json"
								except Exception:
									pass
						except Exception as _mf_ex:
							self.logger.warning(f"Multi-flow manifest generation skipped or failed: {_mf_ex}")
						# END multi-flow manifest emission
						with open("server.env", "w") as fp:
							Parameters.serialize_to_env_file(server_params, fp)
						with open("shaper.env", "w") as fp:
							Parameters.serialize_to_env_file(shaper_params, fp)

						# params += " ".join(testcase.additional_envs())
						# params += " ".join(shaper.additional_envs())
						# params += " ".join(server.additional_envs())
						# containers = "sim server " + " ".join(testcase.additional_containers())
						containers = "sim server tcpdump_leftnet tcpdump_rightnet"

						cmd = (
							docker_compose_vars
							+ " docker compose up -d "
							+ containers
						)
						# self.host_interface.spawn_parallel_subprocess(cmd, False, True)
						_, out, err = self.host_interface.spawn_blocking_subprocess(cmd, False, True) # TODO Test out if this truly fixes the RNETLINK error? This call might be too slow

						self.logger.debug(f"Started sim and server | STDOUT [{out}] | STDERR [{chr(10) if err.find(chr(10)) >= 0 else ''}{err}{chr(10) if err.find(chr(10)) >= 0 else ''}]") # chr(10) => '\n'
						
						# Host applications require some packet rerouting to be able to reach docker containers
						if self.configuration.client_endpoints[client_config["name"]].type == Endpoint.Type.HOST:
							self.logger.debug("Detected local client, rerouting localhost traffic to 193.167.100.0/24 via 193.167.0.2")
							_, out, err = self.host_interface.spawn_blocking_subprocess("ip route del 193.167.100.0/24", True, False)
							if err is not None and len(err) > 0:
								raise VegvisirRunFailedException(f"Failed to remove route to 193.167.100.0/24 | STDOUT [{out}] | STDERR [{err}]")
							self.logger.debug("Removed docker compose route to 193.167.100.0/24")

							_, out, err = self.host_interface.spawn_blocking_subprocess("ip route add 193.167.100.0/24 via 193.167.0.2", True, False)
							if err is not None and len(err) > 0:
								raise VegvisirRunFailedException(f"Failed to reroute 193.167.100.0/24 via 193.167.0.2 | STDOUT [{out}] | STDERR [{err}]")
							self.logger.debug("Rerouted 193.167.100.0/24 via 193.167.0.2")

							_, out, err = self.host_interface.spawn_blocking_subprocess("./veth-checksum.sh", True, False)
							if err is not None and len(err) > 0:
								raise VegvisirRunFailedException(f"Virtual ethernet device checksum failed | STDOUT [{out}] | STDERR [{err}]")								

						# Log kernel/net parameters
						self.print_debug_information("ip address")
						self.print_debug_information("ip route list")
						self.print_debug_information("sysctl -a")
						self.print_debug_information("docker version")
						self.print_debug_information("docker compose version")


						# Setup client
						vegvisirClientArguments = dataclasses.replace(vegvisirBaseArguments, ROLE = "client", TESTCASE = self.configuration.environment.get_QIR_compatibility_testcase(BaseEnvironment.Perspective.CLIENT))
						
						# Decide between single-flow (backward compatible) and multi-flow
						multi_cfg = client_config.get("multi")
						client_cmd = ""
						client_proc = None

						if client.type == Endpoint.Type.DOCKER and isinstance(multi_cfg, dict) and isinstance(multi_cfg.get("flows"), list) and len(multi_cfg["flows"]) > 0:
							# --- Multi-flow mode ---
							self.logger.info(f"Starting {len(multi_cfg['flows'])} client flows sharing the same bottleneck (tc-netem).")
							
							# Prepare per-flow startup plan
							flows = []
							base_args = dict(client_config.get("arguments", {}))
							
							# Ensure per-flow qlog subdirs exist
							qlog_root = os.path.join(self.configuration.path_collection.log_path_client, "qlog")
							pathlib.Path(qlog_root).mkdir(parents=True, exist_ok=True)

							for f in multi_cfg["flows"]:
								label = f.get("label") or f.get("name") or f"flow{len(flows)+1}"
								offset_ms = int(f.get("offset_ms", f.get("start_offset_ms", 0)) or 0)
								flow_args = dict(base_args)
								# Merge per-flow argument overrides (including optional per-flow 'requests')
								for k, v in (f.get("arguments") or {}).items():
									flow_args[k] = v
								if "requests" in f:
									flow_args["requests"] = f["requests"]
								
								# Compute flow-specific hydrated params
								flow_client_args = dataclasses.replace(
									vegvisirClientArguments,
									# Keep base log mapping but segregate qlog/keylog inside the mounted /logs
									QLOGDIR=f"/logs/qlog/{label}/",
									SSLKEYLOGFILE=f"/logs/keys_{label}.log",
								)
								flow_params = client.parameters.hydrate_with_arguments(flow_args, flow_client_args.dict())

								# Ensure the qlog subfolder exists on host for this label
								pathlib.Path(os.path.join(qlog_root, label)).mkdir(parents=True, exist_ok=True)

								flows.append({
									"label": label,
									"offset_ms": offset_ms,
									"params": flow_params,
								})

							# Sort by start offset to schedule staggered starts
							flows.sort(key=lambda x: x["offset_ms"])
							# Compose override: one service per flow with unique static IPs
							flow_labels = [f["label"] for f in flows]
							override_file = self._write_multiflow_compose_override(flow_labels)
							
							# Write per-flow env files referenced by the override
							for f in flows:
								env_path = f"client_{f['label']}.env"
								with open(env_path, "w") as _fp:
									Parameters.serialize_to_env_file(f["params"], _fp)

							# Extend docker compose to include the override file
							compose_file_env = "COMPOSE_FILE=docker-compose.yml:" + override_file + " "
							docker_compose_vars = compose_file_env + docker_compose_vars


							# Start sim/server/tcpdump already running; now spawn each flow container with docker compose run -d
							start_time = time.monotonic()
							running_containers = []  # list of dicts: {name, wait_proc}

							for i, f in enumerate(flows, start=1):
								# staggered start
								target = flows[0]["offset_ms"] if i == 1 else f["offset_ms"]
								delay = (target / 1000.0) - (time.monotonic() - start_time)
								if delay > 0:
									time.sleep(delay)

								# per-flow env for the client service
								with open("client.env", "w") as fp:
									Parameters.serialize_to_env_file(f["params"], fp)

								container_name = f"client_{f['label']}"
								# Ensure old container is gone
								self.host_interface.spawn_blocking_subprocess(f"docker rm -f {container_name}", True, False)

								# Bring up the per-flow service (unique IP via override file)
								run_cmd = docker_compose_vars + f" docker compose up -d --no-recreate {container_name}"
								proc, out, err = self.host_interface.spawn_blocking_subprocess(run_cmd, False, True)
								rc = getattr(proc, "returncode", None)
								if rc is None:
									rc = 0
								if rc != 0:
									self.logger.warning(
										f"Launch of {container_name} returned rc={rc}. Retrying once. STDERR was: {err[:300] if err else ''}"
									)
									self.host_interface.spawn_blocking_subprocess(f"docker rm -f {container_name}", True, False)
									proc, out, err = self.host_interface.spawn_blocking_subprocess(run_cmd, False, True)
									rc = getattr(proc, "returncode", None)
									if rc is None:
										rc = 0
									if rc != 0:
										raise VegvisirRunFailedException(
											f"Failed to start flow container {container_name} | RC [{rc}] | STDOUT [{(out or '')[:300]}] | STDERR [{(err or '')[:300]}]"
										)
								self.logger.debug(f"Started flow container {container_name}.")
								
								# Start a waiter process for this container so sensor system can block on a representative proc if needed
								wait_proc = self.host_interface.spawn_parallel_subprocess(f"docker wait {container_name}", True, False)
								running_containers.append({"name": container_name, "wait_proc": wait_proc})

							# Sensor system: use the last started wait_proc as representative; then wait on all
							representative_proc = running_containers[-1]["wait_proc"] if running_containers else None

							try:
								self.configuration.environment.start_sensors(representative_proc, self.configuration.path_collection)
								# Optional global timeout
								max_dur = int(multi_cfg.get("max_duration_s", 0) or 0)
								if max_dur > 0:
									self.logger.info(f"Global max duration enforced: {max_dur}s. Sensors will unblock when time elapses or flows finish.")
									# Wait for all or until timeout; poll waiters
									deadline = time.monotonic() + max_dur
									while time.monotonic() < deadline and any(p['wait_proc'].poll() is None for p in running_containers):
										time.sleep(1)
								else:
									# Default: wait for all flows to end
									for p in running_containers:
										p['wait_proc'].wait()
								self.configuration.environment.clean_and_reset_sensors()
							except KeyboardInterrupt:
								self.configuration.environment.forcestop_sensors()
								self.configuration.environment.clean_and_reset_sensors()
								with open(os.path.join(self.configuration.path_collection.log_path_permutation, "crashreport.txt"), "w") as fp:
									fp.write("Test aborted by user interaction.")
								self.logger.info("CTRL-C test interrupted")

							# Collect per-flow stdout/stderr and then remove one-off containers
							for f in running_containers:
								name = f["name"]
								stdout_path = os.path.join(self.configuration.path_collection.log_path_client, f"{name}.stdout.log")
								stderr_path = os.path.join(self.configuration.path_collection.log_path_client, f"{name}.stderr.log")
								_, out, err = self.host_interface.spawn_blocking_subprocess(f"docker logs {name}", True, False)
								try:
									with open(stdout_path, "w") as fp:
										fp.write(out or "")
									with open(stderr_path, "w") as fp:
										fp.write(err or "")
								except Exception as ioerr:
									self.logger.warning(f"Failed to write docker logs for {name}: {ioerr}")
								# Cleanup
								self.host_interface.spawn_blocking_subprocess(f"docker rm -f {name}", True, False)

						else:
							# --- Single-flow (backward compatible) ---
							client_params = client.parameters.hydrate_with_arguments(client_config.get("arguments", {}), vegvisirClientArguments.dict())
							if client.type == Endpoint.Type.DOCKER:
								with open("client.env", "w") as fp:
									Parameters.serialize_to_env_file(client_params, fp)
								
								# Docker compose 2.17 introduced a breaking change: --timeout was renamed --waitTimeout
								# Both, however, support the -t shorthand which we will use to be backwards compatible with previous docker compose versions
								client_cmd = (
									docker_compose_vars
									+ " docker compose up --abort-on-container-exit -t 1 "
									+ "client"
								)
								client_proc = self.host_interface.spawn_parallel_subprocess(client_cmd, False, True)

							elif client.type == Endpoint.Type.HOST:
								for constructor in client.construct:
									constructor_command = constructor.serialize_command(client_params)
									self.logger.debug(f"Issuing client construct command [{constructor_command}]")
									_, out, err = self.host_interface.spawn_blocking_subprocess(constructor_command, constructor.requires_root, True)
									if out is not None and len(out) > 0:
										self.logger.debug(f"Construct command STDOUT:\\n{out}")
									if err is not None and len(err) > 0:
										self.logger.debug(f"Construct command STDERR:\\n{err}")
								client_cmd = client.command.serialize_command(client_params)
								client_proc = self.host_interface.spawn_parallel_subprocess(client_cmd)

							self.logger.debug("Vegvisir: running client: %s", client_cmd)

							try:
								self.configuration.environment.start_sensors(client_proc, self.configuration.path_collection)
								self.configuration.environment.waitfor_sensors()
								self.configuration.environment.clean_and_reset_sensors()
							except KeyboardInterrupt:
								self.configuration.environment.forcestop_sensors()
								self.configuration.environment.clean_and_reset_sensors()
								with open(os.path.join(self.configuration.path_collection.log_path_permutation, "crashreport.txt"), "w") as fp:
									fp.write("Test aborted by user interaction.")
								self.logger.info("CTRL-C test interrupted")
                                
							if client_proc is not None:
								client_proc.terminate()  # TODO redundant?
								if client.type == Endpoint.Type.HOST:
									# Doing this for docker will nullify the sensor system
									# Docker client logs are retrieved with "docker compose logs"
									out, err = client_proc.communicate()
									self.logger.debug(out.decode("utf-8"))
									self.logger.debug(err.decode("utf-8"))
						_, out, err = self.host_interface.spawn_blocking_subprocess(docker_compose_vars + " docker compose logs --timestamps server", False, True)
						self.logger.debug(out)
						self.logger.debug(err)
						_, out, err = self.host_interface.spawn_blocking_subprocess(docker_compose_vars + " docker compose logs --timestamps sim", False, True)
						self.logger.debug(out)
						self.logger.debug(err)
						_, out, err = self.host_interface.spawn_blocking_subprocess(docker_compose_vars + " docker compose logs --timestamps client", False, True)
						self.logger.debug(out)
						self.logger.debug(err)
						_, out, err = self.host_interface.spawn_blocking_subprocess(docker_compose_vars + " docker compose logs --timestamps tcpdump_leftnet", False, True)
						self.logger.debug(out)
						self.logger.debug(err)
						_, out, err = self.host_interface.spawn_blocking_subprocess(docker_compose_vars + " docker compose logs --timestamps tcpdump_rightnet", False, True)
						self.logger.debug(out)
						self.logger.debug(err)

						_, out ,err = self.host_interface.spawn_blocking_subprocess(docker_compose_vars + " docker compose down", False, True) # TODO TEMP
						self.logger.debug(out)
						self.logger.debug(err)

					# BREAKDOWN
					if client.type == Endpoint.Type.HOST:
						for destructor in client.destruct:
								destructor_command = destructor.serialize_command(client_params)
								self.logger.debug(f"Issuing client destruct command [{destructor_command}]")
								_, out, err = self.host_interface.spawn_blocking_subprocess(destructor_command, destructor.requires_root, True)
								if out is not None and len(out) > 0:
									self.logger.debug(f"Destruct command STDOUT:\n{out}")
								if err is not None and len(err) > 0:
									self.logger.debug(f"Destruct command STDERR:\n{err}")

						_, out, err = self.host_interface.spawn_blocking_subprocess("hostman remove --names=server4", True, False)
						self.logger.debug("Vegvisir: remove entry from hosts: %s", out.strip())
						if err is not None and len(err) > 0:
							self.logger.debug("Vegvisir: removing entry from hosts file resulted in error: %s", err)

					# Change ownership of docker output to running user
					try:
						real_username = getpass.getuser()
						real_primary_groupname = grp.getgrgid(os.getgid()).gr_name
						chown_to = f"{real_username}:{real_primary_groupname}"
						_, out, err = self.host_interface.spawn_blocking_subprocess(f"chown -R {chown_to} {self.configuration.path_collection.log_path_permutation}", True, False)
						if len(err) > 0:
							raise VegvisirException(err)
						self.logger.debug(f"Changed ownership of output logs to {chown_to} | {self.configuration.path_collection.log_path_permutation}")
					except (KeyError, TypeError):
						self.logger.warning(f"Could not change log output ownership @ {self.configuration.path_collection.log_path_permutation}, groupname might not be found?")
					except VegvisirException as e:
						self.logger.warning(f"Could not change log output ownership [{e}] @ {self.configuration.path_collection.log_path_permutation}")

					self.post_hook_processor_queue.put((self.configuration.environment.post_run_hook, path_collection_copy))  # Queue is infinite, should not block

					experiment_permutation_counter += 1

					if self.configuration.iterations > 1:
						self.logger.info(f'Test run {run_number}/{self.configuration.iterations} duration: {datetime.now() - iteration_start_time}')
					else:
						self.logger.info(f'Test run duration: {datetime.now() - iteration_start_time}')
					
					self.logger.removeHandler(log_handler)
					log_handler.close()
		
		yield None, None, None, None, None

		# Halt the hook processors
		wait_for_hook_processors_counter = 0
		while True:
			if self.post_hook_processor_queue.qsize() == 0:
				self.post_hook_processor_request_stop = True
			states = [t.is_alive() for t in self.post_hook_processors]
			time.sleep(5)
			if not any(states):
				for t in self.post_hook_processors:
					t.join()
				break
			if wait_for_hook_processors_counter % 2 == 0:
				hooks_todo = self.post_hook_processor_queue.qsize()
				if hooks_todo > 0:
					self.logger.info(f"Vegvisir is waiting for all post-hooks to process, approximately {self.post_hook_processor_queue.qsize()} request(s) still in queue.")
				else:
					self.logger.info(f"Vegvisir is waiting for {sum(states)} post-hook processor(s) to stop. If this message persists, perform CTRL + C")
			wait_for_hook_processors_counter += 1